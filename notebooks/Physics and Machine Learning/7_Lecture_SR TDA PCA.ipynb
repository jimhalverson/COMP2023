{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Symbolic Regression\n",
                "\n",
                "In this lecture we study symbolic regression, which is a method for finding a symbolic expression that fits a given data set."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### $ F \\propto 1/r^2$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let's try this first on Fs."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Very clean. Exactly as expected.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "OK, so it may or may not have worked again, depending on the seed. But presumably if we added TOO much noise, it would fail eventually."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Notice the stratification in the loss. It starts very high, but then gets much lower by the time we get to 1/x^2. This shows we're not getting a lot more beyond the 1/x^2, so balancing complexity and loss, this is the best model."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Damped Harmonic Oscillator"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "A * np.exp(-beta * times) * np.cos(omega * times - delta)\n",
                "\n",
                "A_true = 1.4\n",
                "\n",
                "beta_true = 0.3\n",
                "\n",
                "omega_true = np.pi**2 \n",
                "\n",
                "delta_true = 0.3"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Topological Data Analysis"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now we study topological data analysis. The essential question is, if your experiments or simulations sampled a manifold with non-trivial topology, how would you know it from a finite number of samples?\n",
                "\n",
                "In the figure below at small values of r, we simply see a collection of points. But they're clearly arranged in a way that makes it looks like they're sampled from either a collection of circles, or a cylinder -- it's not simply a connection of point. As we increase $r$, which in this video is simply the marker size, eventually everything overlaps and it's one big blob. Intuitively, that is also not the lesson that we're supposed to take away from the data."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "Roughly, a non-trivial $k$-cycle in a space is a $k$-dimensional subspace without boundary that is not the boundary of a $(k+1)$-dimensional subspace. For example, a torus has two one-cycles, which are the two circles that go around the torus. A two-sphere has no one-cycles, but it has a two-cycle, which is the entire sphere. Given a topological space, the dimensionality of the $k$-cycles is given by the so-called Betti numbers.\n",
                "\n",
                "The basic idea in persistent homology is to study how the Betti numbers change as a function of a so-called filtration parameter, $r$. \n",
                "\n",
                "In slightly more detail, to any value of $r$ we may associate a so-called Vietoris-Rips simplicial complex. Recall that a $k$-simplex is a $k$-dimensional subspace. A simplicial complex is a collection of simplices that is closed under taking subspaces. The Vietoris-Rips complex is the simplicial complex that is generated by the points in the data set, where we include a $k$-simplex if all of its vertices are within distance $r$ of each other. We see that it is clearly closed under taking subspaces because if we have a $k$-simplex, then all of its vertices are within distance $r$ of each other, and so are all of the vertices of any subspace of the $k$-simplex.\n",
                "\n",
                "We will be using the `ripser` package. The basic idea is that we can study the topology of a data set by looking at the persistence of the homology of the Vietoris-Rips complex of the data set."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "For a fixed number of samples, having a smaller lower_radius makes the samples look like a blob. Accordingly, in the TDA you can't see a meaningful one-cycle. But as the lower_radius increases, the one-cycle becomes more and more visible, as evidenced by the $H_1$ gap in the persistence diagram."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# PCA: Principal Component Analysis\n",
                "\n",
                "In this lecture we study PCA, which is a method for finding a low-dimensional subspace that captures the most variance in a data set. The principal components are eigenvectors of the covariance matrix. The idea is that the different principle components explain the variance in the data. \n",
                "\n",
                "But how much of the variance is explained by each component? If the $i^\\text{th}$ principal component is the $i^{th}$ eigenvector of the covariance matrix, then the size of its eigenvalue (relative to the sum of all of them) tells you \"how much\" of the variance is explained by that component.\n",
                "\n",
                "In equations\n",
                "\n",
                "$$\n",
                "\\text{variance explained by $i^\\text{th}$ component} = \\frac{\\lambda_i}{\\sum_j \\lambda_j}\n",
                "$$\n",
                "\n",
                "where $\\lambda_i$ is the $i^\\text{th}$ eigenvalue of the covariance matrix.\n",
                "\n",
                "This normalized sum is called the explained variance ratio.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We have successfully created a dataset that is rotated and has a clear principal component. Let's see if we can find it by computing PCA by hand."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The princpal components are the eigenvectors of the covariance matrix. Let's study them."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "This matches expectations because we see the (-1,-1) direction gets weight 10 and the (-1,1) direction gets weight 1. So the principal component is the (-1,-1) direction. The precisely matches what we would have expected from `R` and `diag_cov` above.\n",
                "\n",
                "Let's make sure a pre-packaged library will give us the same thing."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We see that the explained variance and the principal components match what we computed by hand. Yes!"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "COMP",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.4"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}