{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Statistics and Monte Carlo Integration\n",
                "\n",
                "In this lecture we will review the basics of statistics and perform some Monte Carlo integration. The goal is to lay the foundation for doing various statistical mechanics and quantum mechanics problems on a computer."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Drawing Random Numbers: Uniform Distribution\n",
                "\n",
                "``Random\" numbers on a computer are generated by *pseudo* random number generators (PRNGs). For the most part we won't worry about this in this class, but I should say it once. For instance, the algorithm below is one way to draw pseudo-random uniform numbers. Some particular values denote the \"RANDU\" algorithm, which is a particularly bad PRNG, as discussed in the [Wikipedia article](https://en.wikipedia.org/wiki/RANDU)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Non-uniform random distributions\n",
                "--------------------------------\n",
                "\n",
                "In the previous section we learned how to generate random deviates with\n",
                "a uniform probability distribution in an interval $[a,b]$. This\n",
                "distributioon is normalized, so that $$\\int _a^b {P(x)dx}=1.$$ Hence,\n",
                "$P(x)=1/(b-a)$.\n",
                "\n",
                "Now, suppose that we generate a sequence $\\{x_i\\}$ and we take some\n",
                "function of it to generate $\\{y(x_i)\\}=\\{y_i\\}$. This new sequence is\n",
                "going to be distributed according to some probability density $P(y)$,\n",
                "such that $$P(y)dy=P(x)dx$$ or $$P(y)=P(x)\\frac{dx}{dy}.$$\n",
                "\n",
                "If we want to generate a desired normalized distribution $P(y)$, we need\n",
                "to solve the differential equation: $$\\frac{dx}{dy}=P(y).$$ But the\n",
                "solution of this is $$x=\\int _0^y {P(y')dy'}=F(y).$$ Therefore,\n",
                "$$y(x)=F^{-1}(x),\n",
                "$$ where $F^{-1}$ is the inverse of $F$.\n",
                "\n",
                "### Exponential distribution\n",
                "\n",
                "As an example, let us take $y(x)=-\\ln{(x)}$ with $P(x)$ representing a\n",
                "uniform distribution in the interval $[0,1]$. Then\n",
                "$$P(y)=\\frac{dx}{dy}=e^{-y},$$ \n",
                "which is distributed exponentially. This\n",
                "distribution occurs frequently in real problems such as the radioactive\n",
                "decay of nuclei. You can also see that the quantity $y/\\lambda$ has the\n",
                "distribution $\\lambda\n",
                "e^{-\\lambda y}$.\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "You can get Gaussian random variables by the so-called Box-Muller transform. We'll demonstrate that it *works*, but for a thorough introduction read here: [Box-Muller Wikipedia](https://en.wikipedia.org/wiki/Box%E2%80%93Muller_transform)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "von Neumann rejection sampling\n",
                "------------------------------\n",
                "\n",
                "Rejection sampling was invented by von Neumann in 1951. It is a simple\n",
                "and general method for generating arbitrary distributions. The idea is\n",
                "to sample uniformly from a region known to enclose the distribution of\n",
                "interest, and to reject any samples that lie outside the region.\n",
                "\n",
                "To do rejection sampling, draw a plot\n",
                "with you probability distribution, and on the same graph, plot another\n",
                "curve $f(x)$ which has finite area and lies everywhere above your\n",
                "original distribution. We will call $f(x)$ the \u201ccomparison function\u201d.\n",
                "Generate random pairs $(x_i,y_i)$ with uniform distribution inside\n",
                "$f(x)$. Whenever the point lies inside the area of the original\n",
                "probability, we accept it, otherwise, we reject it. All the accepted\n",
                "points will be uniformly distributed within the original area, and\n",
                "therefore will have the desired distribution. The fraction of points\n",
                "accepted/rejected will deppend on the ratio between the two areas. The\n",
                "closer the comparison function $f(x)$ resembles $P(x)$, the more points\n",
                "will be accepted. Ideally, for $P(x)=f(x)$, all the points will be\n",
                "accepted, and none rejected. However, in practice, this is not always\n",
                "possible, but we can try to pick $f(x)$ such that we minimize the\n",
                "fraction of rejected points.\n",
                "\n",
                "You know a version of this from your first homework: you computed $\\pi$ by\n",
                "drawing random points in a square between $0$ and $1$ and counting how many were inside a\n",
                "circle. The circle is the region enclosing the distribution of interest, the uniform density on the circle, and the square is the region from which you can easily draw, as shown above. By rejection sampling, you computed the volume of the circle relative to the volume of the square. That ratio is $\\pi/4$."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Random walk methods: the Metropolis algorithm\n",
                "---------------------------------------------\n",
                "\n",
                "Suppose that we want to generate random variables according to an\n",
                "arbitrary probability density $P(x)$. The Metropolis algorithm produces\n",
                "a \u201crandom walk\u201d of points $\\{x_i\\}$ whose asymptotic probability\n",
                "approaches $P(x)$ after a large number of steps. The random walk is\n",
                "defined by a \u201ctransition probability\u201d $w(x_i \\rightarrow x_j)$ for one\n",
                "value $x_i$ to another $x_j$ in order that the distribution of points\n",
                "$x_0$, $x_1$, $x_2$, ... converges to $P(x)$. In can be shown that it is\n",
                "sufficient (but not necessary) to satisfy the \u201cdetailed balance\u201d\n",
                "condition \n",
                "$$p(x_i)w(x_i \\rightarrow x_j) = p(x_j)w(x_j \\rightarrow x_i).$$ \n",
                "This relation dos not specify $w(x_i \\rightarrow x_j)$\n",
                "uniquely. A simple choice is\n",
                "$$w(x_i \\rightarrow x_j)=\\min{\\left[ 1,\\frac{P(x_j)}{P(x_i)} \\right] }.$$\n",
                "This choice can be described by the following steps. Suppose that the\n",
                "\u201crandom walker\u201d is a position $x_n$. To generate $x_{n+1}$ we\n",
                "\n",
                "1.  choose a trial position $x_t=x_n+\\delta _n$ , where the $\\delta _n$\n",
                "    is a random number in the interval $[-\\delta ,\\delta]$.\n",
                "\n",
                "2.  Calculate $w=P(x_t)/P(x_n)$.\n",
                "\n",
                "3.  If $w \\geq 1$ we accept the change and let $x_{n+1}=x_t$.\n",
                "\n",
                "4.  If $w \\leq 1$, generate a random number $r$.\n",
                "\n",
                "5.  If $r \\leq w$, accept the change and let $x_{n+1} = x_t$.\n",
                "\n",
                "6.  If the trial change is not accepted, the let $x_{n+1}=x_n$.\n",
                "\n",
                "It is necessary to sample a number of points of the random walk before\n",
                "the asymptotic probability $P(x)$ is attained. How do we choose the\n",
                "\u201cstep size\u201d $\\delta$? If $\\delta$ is too large, only a small fraction of\n",
                "changes will be accepted and the sampling will be inefficient. If\n",
                "$\\delta$ is too small, a large number will be accepted, but it would\n",
                "take too long to sample $P(x)$ over the whole interval of interest.\n",
                "Ideally, we want at least 1/3-1/2 of the trial steps to be accepted. We\n",
                "also want to choose $x_0$ such that the distribution $\\{x_i\\}$ converges\n",
                "to $P(x)$ as quickly as possible. An obvious choice is to begin the\n",
                "random walk at the point where $P(x)$ is maximum.\n",
                "\n",
                "### Exercise 9.1: The Gaussian distribution\n",
                "\n",
                "1.  Use the Metropolis algorithm to generate a Gaussian distribution\n",
                "    $P(x)=A \\exp{(-x^2/2\\sigma ^2)}$. Is the numerical value of the\n",
                "    normalization constant $A$ relevant? Determine the qualitative\n",
                "    dependence of the acceptance ratio and the equilibrium time on the\n",
                "    maximum step size $\\delta$. One possible criterion for equilibrium\n",
                "    is that $\\langle x^2\n",
                "    \\rangle \\approx \\sigma ^2$. For $\\sigma = 1$, what is a reasonable\n",
                "    choice of $\\delta$? (choose $x_0 = 0$.)\n",
                "\n",
                "2.  Plot the asymptotic probability distribution generated by the\n",
                "    Metropolis algorithm.\n",
                "    "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "collapsed": true
            },
            "source": [
                "#### Challenge 9.2:\n",
                "\n",
                "- Modify the code above to study the equilibration \"time\" for different step size $\\delta$. \n",
                "\n",
                "- Analize the acceptance ratio in terms of $\\delta$."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Monte Carlo integration\n",
                "=======================\n",
                "\n",
                "Imagine that we want to measure the area of a pond with arbitrary shape.\n",
                "Suppose that this pond is in the middle of a field with known area $A$.\n",
                "If we throw $N$ stones randomly, such that they land within the\n",
                "boundaries of the field, and we count the number of stones that fall in\n",
                "the pond $N_{in}$, the area of the pond will be approximately\n",
                "proportional to the fraction of stones that make a splash, multiplied by\n",
                "$A$: $$A_{pond}=\\frac{N_{in}}{N}A.$$ This simple procedure is an example\n",
                "of the \u201cMonte Carlo\u201d method.\n",
                "\n",
                "Simple Monte Carlo integration\n",
                "------------------------------\n",
                "\n",
                "More generaly, imagine a rectangle of height $H$ in the integration\n",
                "interval $[a,b]$, such that the function $f(x)$ is within its\n",
                "boundaries. Compute $n$ pairs of random numbers $(x_i,y_i)$ such that\n",
                "they are uniformly distributed inside this rectangle. The fraction of\n",
                "points that fall within the area contained below $f(x)$, <span>*i.\n",
                "e.*</span>, that satisfy $y_i \\leq f(x_i)$ is an estimate of the ratio o\n",
                "fthe integral of $f(x)$ and the area of the rectangle. Hence, the\n",
                "estimate of the integral will be given by:\n",
                "$$\\int _a^b{f(x)dx} \\simeq I(N) = \\frac{N_{in}}{N}H(b-a).\n",
                "$$\n",
                "\n",
                "Another Monte Carlo procedure is based on the definition:\n",
                "$$\\langle f \\rangle=\\frac{1}{(b-a)} \\int _a^b{f(x)dx}.$$ \n",
                "In order to determine this average, we sample the\n",
                "value of $f(x)$:\n",
                "$$\\langle f \\rangle \\simeq \\frac{1}{N}\\sum_{i=1}^{N}f(x_i),$$ \n",
                "where the\n",
                "$N$ values $x_i$ are distributed unformly in the interval $[a,b]$. The\n",
                "integral will be given by $$I(N)=(b-a) \\langle f \\rangle .$$\n",
                "\n",
                "Monte Carlo error analysis\n",
                "--------------------------\n",
                "\n",
                "The Monte Carlo method clearly yields approximate results. The accuracy\n",
                "deppends on the number of values $N$ that we use for the average. A\n",
                "possible measure of the error is the \u201cvariance\u201d $\\sigma^2$ defined by:\n",
                "$$\\sigma ^2=\\langle f^2 \\rangle - \\langle f \\rangle ^2,$$ \n",
                "where\n",
                "$$\\langle f \\rangle = \\frac{1}{N} \\sum_{i=1}^N f(x_i)$$ \n",
                "and\n",
                "$$\\langle f^2 \\rangle = \\frac{1}{N} \\sum_{i=1}^{N} f(x_i)^2.$$ \n",
                "The\n",
                "\u201cstandard deviation\u201d is $\\sigma$. However, we should expect that the\n",
                "error decreases with the number of points $N$, and the quantity $\\sigma$\n",
                "defines by (\\[mc\\_sigma\\]) does not. Hence, this cannot be a good\n",
                "measure of the error.\n",
                "\n",
                "Imagine that we perform several measurements of the integral, each of\n",
                "them yielding a result $I_n$. These values have been obtained with\n",
                "different sequences of $N$ random numbers. According to the central\n",
                "limit theorem, these values whould be normally dstributed around a mean\n",
                "$\\langle I\n",
                "\\rangle$. Suppouse that we have a set of $M$ of such measurements\n",
                "${I_n}$. A convenient measure of the differences of these measurements\n",
                "is the \u201cstandard deviation of the means\u201d $\\sigma_M$:\n",
                "$$\\sigma_M ^2=\\langle I^2 \\rangle - \\langle I \\rangle ^2,$$ \n",
                "where\n",
                "$$\\langle I \\rangle = \\frac{1}{M} \\sum_{n=1}^M I_n$$ \n",
                "and\n",
                "$$\\langle I^2 \\rangle = \\frac{1}{M} \\sum_{n=1}^{M} I_n^2.$$ \n",
                "It can be proven that\n",
                "$$\\sigma_M \\approx \\sigma/\\sqrt{N}.$$ \n",
                "This relation becomes exact in the limit of a very\n",
                "large number of measurements. Note that this expression implies that the\n",
                "error decreases with the square root of the number of trials, meaning\n",
                "that if we want to reduce the error by a factor 10, we need 100 times\n",
                "more points for the average.\n",
                "\n",
                "# Example: Gaussian moments\n",
                "To illustrate some of the these ideas we will utilize some Gaussian integrals. We write the Gaussian distribution as \n",
                "$$\n",
                "P(x) = \\exp(-a x^2/2)\n",
                "$$ \n",
                "up to a normalization factor that must be imposed. With this convention, the $2n$ moment is given by\n",
                "$$\n",
                "\\langle x^{2n} \\rangle = \\frac{(2n-1)!!}{a^n}\n",
                "$$ \n",
                "where $n!!$ is the double factorial $n(n-2)(n-4)\\dots 1$ and $(2n-1)!!$ admits a graphical interpretation as the number of ways to connect $2n$ points in pairs. This relation is at the foundation of Feynman diagram combinatorics in quantum field theory.\n",
                "\n",
                "We wish to demonstrate that we can do these moment calculations via Monte Carlo integration. We will pay particular attention to see convergence happening, and will also study \"negative moments\" which are divergent integrals; clearly Monte Carlo integration won't work in that case.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Variance reduction\n",
                "------------------\n",
                "\n",
                "If the function being integrated does not fluctuate too much in the\n",
                "interval of integration, and does not differ much from the average\n",
                "value, then the standard Monte Carlo mean-value method should work well\n",
                "with a reasonable number of points. Otherwise, we will find that the\n",
                "variance is very large, meaning that some points will make small\n",
                "contributions, while others will make large contributions to the\n",
                "integral. If this is the case, the algorithm will be very inefficient.\n",
                "The method can be improved by splitting the function $f(x)$ in two\n",
                "$f(x)=f_1(x)+f_2(x)$, such that the integral of $f_1(x)$ is known, and\n",
                "$f_2(x)$ as a small variance. The \u201cvariance reduction\u201d technique,\n",
                "consists then in evaluating the integral of $f_2(x)$ to obtain:\n",
                "$$\\int _a^b{f(x)dx}=\\int _a^b {f_1(x)dx} + \\int _a^b{f_2(x)dx} = \\int\n",
                "_a^b{f_1(x)dx}+J.$$\n",
                "\n",
                "Importance Sampling\n",
                "-------------------\n",
                "\n",
                "Imagine that we want to sample the function $f(x)=e^{-x^2}$ in the\n",
                "interval $[0,1]$. It is evident that most of our points will fall in the\n",
                "region where the value of $f(x)$ is very small, and therefore we will\n",
                "need a large number of values to achieve a decent accuracy. A way to\n",
                "improve the measurement by reducing the variance is obtained by\n",
                "\u201cimportance sampling\u201d. As the name says, the idea is to sample the\n",
                "regions with larger contributions to the integral. For this goal, we\n",
                "introduce a probability distribution $P(x)$ normalized in the interval\n",
                "of integration $$\\int _a^b{P(x)dx} = 1.$$ Then, we can rewrite the\n",
                "integral of $f(x)$ as \n",
                "$$I=\\int _a^b{\\frac{f(x)}{P(x)}P(x)dx}$$ \n",
                "We can evaluate this integral, by sampling\n",
                "according to the probability distribution $P(x)$ and evaluating the sum\n",
                "$$I(N)=\\frac{1}{N} \\sum_{i=1}^N \\frac{f(x_i)}{P(x_i)}.$$ \n",
                "Note that for the uniform case $P(x)=1/(b-a)$, the\n",
                "expression reduces to the simple Monte Carlo integral.\n",
                "\n",
                "We are free to choose $P(x)$ now. We wish to do it in a way to reduce\n",
                "and minimize the variance of the integrand $f(x)/P(x)$. The way to to\n",
                "this is picking a $P(x)$ that mimics $f(x)$ where $f(x)$ is large. if we\n",
                "are able to determine an apropiate $P(x)$, the integrand will be slowly\n",
                "varying, and hence the variance will be reduced. Another consideration\n",
                "is that the generation of points according to the distribution $P(x)$\n",
                "should be a simple task. As an example, let us consider again the\n",
                "integral \n",
                "$$I=\\int _0^1 {e^{-x^2}dx}.$$ \n",
                "A reasonable choice for a weigh\n",
                "function is $P(x)=Ae^{-x}$, where $A$ is a normalization constant.\n",
                "\n",
                "Notice that for $P(x)=f(x)$ the variance is zero! This is known as the\n",
                "zero variance property. There is a catch, though: The probability function\n",
                "$P(x)$ needs to be normalized, implying that in reality, $P(x)=f(x)/\\int f(x)dx$, which\n",
                "assumes that we know in advance precisely the integral that we are trying to calculate!\n",
                "\n",
                "### Exercise 10.3: Importance sampling \n",
                "\n",
                "1.  Choose the weight function $P(x)=e^{-x}$ and evaluate the integral:\n",
                "    $$\\int _0^{\\infty} {x^{3/2}e^{-x}dx}.$$\n",
                "\n",
                "2.  Choose $P(x)=e^{-ax}$ and estimate the integral\n",
                "    $$\\int _0^{\\pi} \\frac{dx}{x^2+\\cos ^2{x}}.$$ \n",
                "    Determine the value of\n",
                "    $a$ that minimizes the variance of the integral.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Exercise 10.4: The Metropolis algorithm \n",
                "\n",
                "Use the Metropolis algorithm to sample points according to a ditribution\n",
                "and estimate the integral $$\\int _0^4 {x^2e^{-x}dx},$$ with\n",
                "$P(x)=e^{-x}$ for $0 \\leq x \\leq 4$. Plot the number of times the\n",
                "walker is at points $x_0$, $x_1$, $x_2$, ... Is the integrand sampled\n",
                "uniformly? If not, what is the approximate region of $x$ where the\n",
                "integrand is sampled more often?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Challenge 10.1\n",
                "\n",
                "- Calculate the integral $\\int_0^1 x^2 dx=1/3$ using simple MC integration and importance sampling with $P(x)=x$.\n",
                "\n",
                "- Calculate the integral $\\int_0^1 \\sqrt{x}dx=2/3$ using simple MC integration and $P(x)=1-e^{-ax}$. Find the values of $a$ that minimizes the variance."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.4"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}